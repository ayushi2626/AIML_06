{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport os\nimport torch\nimport time\nimport torchvision\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torchvision.datasets.utils import download_url\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as tt\nfrom torch.utils.data import random_split\nfrom torchvision.utils import make_grid\nimport torchvision.models as models\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import *\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:29:49.622126Z","iopub.execute_input":"2023-11-26T10:29:49.622383Z","iopub.status.idle":"2023-11-26T10:29:54.275813Z","shell.execute_reply.started":"2023-11-26T10:29:49.622360Z","shell.execute_reply":"2023-11-26T10:29:54.274864Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 400\nepochs = 120\nmax_lr = 0.001\ngrad_clip = 0.01\nweight_decay =0.001\nopt_func = torch.optim.Adam","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:29:54.277850Z","iopub.execute_input":"2023-11-26T10:29:54.278591Z","iopub.status.idle":"2023-11-26T10:29:54.282714Z","shell.execute_reply.started":"2023-11-26T10:29:54.278565Z","shell.execute_reply":"2023-11-26T10:29:54.281806Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_data = torchvision.datasets.CIFAR100('./', train=True, download=True)\n\nx = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))]) #1600000 X 32 X 3 array\n \nmean = np.mean(x, axis=(0, 1))/255\nstd = np.std(x, axis=(0, 1))/255\n \nmean=mean.tolist()\nstd=std.tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:29:54.284499Z","iopub.execute_input":"2023-11-26T10:29:54.284821Z","iopub.status.idle":"2023-11-26T10:30:11.872218Z","shell.execute_reply.started":"2023-11-26T10:29:54.284797Z","shell.execute_reply":"2023-11-26T10:30:11.871066Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./cifar-100-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 169001437/169001437 [00:06<00:00, 26205688.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./cifar-100-python.tar.gz to ./\n","output_type":"stream"}]},{"cell_type":"code","source":"train_transform = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n                         tt.RandomHorizontalFlip(), \n                         tt.ToTensor(), \n                         tt.Normalize(mean,std,inplace=True)])\ntest_transform = tt.Compose([tt.ToTensor(), tt.Normalize(mean,std)])","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:11.874338Z","iopub.execute_input":"2023-11-26T10:30:11.874655Z","iopub.status.idle":"2023-11-26T10:30:11.880182Z","shell.execute_reply.started":"2023-11-26T10:30:11.874618Z","shell.execute_reply":"2023-11-26T10:30:11.879308Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"trainset = torchvision.datasets.CIFAR100(\"./\",\n                                         train=True,\n                                         download=True,\n                                         transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(\n    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n\ntestset = torchvision.datasets.CIFAR100(\"./\",\n                                        train=False,\n                                        download=True,\n                                        transform=test_transform)\ntestloader = torch.utils.data.DataLoader(\n    testset, batch_size*2,pin_memory=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:11.881375Z","iopub.execute_input":"2023-11-26T10:30:11.881716Z","iopub.status.idle":"2023-11-26T10:30:13.611764Z","shell.execute_reply.started":"2023-11-26T10:30:11.881685Z","shell.execute_reply":"2023-11-26T10:30:13.610825Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Device Setup","metadata":{}},{"cell_type":"code","source":"def get_default_device():\n    \"\"\"Pick GPU if available, else CPU\"\"\"\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n    \ndef to_device(data, device):\n    \"\"\"Move tensor(s) to chosen device\"\"\"\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dl: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dl)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:13.612888Z","iopub.execute_input":"2023-11-26T10:30:13.613154Z","iopub.status.idle":"2023-11-26T10:30:13.621118Z","shell.execute_reply.started":"2023-11-26T10:30:13.613130Z","shell.execute_reply":"2023-11-26T10:30:13.620075Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"device = get_default_device()\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:13.622049Z","iopub.execute_input":"2023-11-26T10:30:13.622366Z","iopub.status.idle":"2023-11-26T10:30:13.654130Z","shell.execute_reply.started":"2023-11-26T10:30:13.622334Z","shell.execute_reply":"2023-11-26T10:30:13.653294Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"trainloader = DeviceDataLoader(trainloader, device)\ntestloader = DeviceDataLoader(testloader, device)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:30:13.655166Z","iopub.execute_input":"2023-11-26T10:30:13.655947Z","iopub.status.idle":"2023-11-26T10:30:13.667995Z","shell.execute_reply.started":"2023-11-26T10:30:13.655922Z","shell.execute_reply":"2023-11-26T10:30:13.667180Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Layer","metadata":{}},{"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1)\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n\nclass ImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, labels = batch \n        out = self(images)                  # Generate predictions\n        loss = F.cross_entropy(out, labels) # Calculate loss\n        return loss\n    \n    def validation_step(self, batch):\n        images, labels = batch \n        out = self(images)                    # Generate predictions\n        loss = F.cross_entropy(out, labels)   # Calculate loss\n        acc = accuracy(out, labels)           # Calculate accuracy\n        return {'val_loss': loss.detach(), 'val_acc': acc}\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n        \ndef conv_block(in_channels, out_channels, pool=False):\n    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n              nn.BatchNorm2d(out_channels), \n              nn.ReLU(inplace=True)]\n    if pool: layers.append(nn.MaxPool2d(2))\n    return nn.Sequential(*layers)\n\nclass ResNet9(ImageClassificationBase):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        \n        self.conv1 = conv_block(in_channels, 64)\n        self.conv2 = conv_block(64, 128, pool=True) \n        \n        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n        \n        self.conv3 = conv_block(128, 256, pool=True)\n        self.conv4 = conv_block(256, 512, pool=True) \n        \n        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) \n        self.conv5 = conv_block(512, 1028, pool=True) \n        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))  \n        \n        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n                                        nn.Flatten(), # 1028 \n                                        nn.Linear(1028, num_classes)) # 1028 -> 100\n        \n    def forward(self, xb):\n        out = self.conv1(xb)\n        out = self.conv2(out)\n        out = self.res1(out) + out\n        out = self.conv3(out)\n        out = self.conv4(out)\n        out = self.res2(out) + out\n        out = self.conv5(out)\n        out = self.res3(out) + out\n        out = self.classifier(out)\n        return out\n\nmodel = to_device(ResNet9(3, 100), device)\nmodel","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:58:58.002407Z","iopub.execute_input":"2023-11-26T10:58:58.003163Z","iopub.status.idle":"2023-11-26T10:58:58.343412Z","shell.execute_reply.started":"2023-11-26T10:58:58.003123Z","shell.execute_reply":"2023-11-26T10:58:58.342495Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"ResNet9(\n  (conv1): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res1): Sequential(\n    (0): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (conv3): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res2): Sequential(\n    (0): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (conv5): Sequential(\n    (0): Conv2d(512, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU(inplace=True)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (res3): Sequential(\n    (0): Sequential(\n      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n    (1): Sequential(\n      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (2): ReLU(inplace=True)\n    )\n  )\n  (classifier): Sequential(\n    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (1): Flatten(start_dim=1, end_dim=-1)\n    (2): Linear(in_features=1028, out_features=100, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Training The Model","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, test_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in test_loader]\n    return model.validation_epoch_end(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n\ndef fit_one_cycle(epochs, max_lr, model, train_loader, test_loader, \n                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n   \n    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n \n    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n                                                steps_per_epoch=len(train_loader))\n    \n    for epoch in range(epochs):\n        # Training Phase \n        model.train()\n        train_losses = []\n        lrs = []\n        for batch in train_loader:\n            loss = model.training_step(batch)\n            train_losses.append(loss)\n            loss.backward()\n            \n           \n            if grad_clip: \n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n           \n            lrs.append(get_lr(optimizer))\n            sched.step()\n        \n        result = evaluate(model, test_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        result['lrs'] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    return history","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:59:02.074554Z","iopub.execute_input":"2023-11-26T10:59:02.075249Z","iopub.status.idle":"2023-11-26T10:59:02.085997Z","shell.execute_reply.started":"2023-11-26T10:59:02.075212Z","shell.execute_reply":"2023-11-26T10:59:02.085018Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"h = [evaluate(model, testloader)]\nh","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:59:03.184976Z","iopub.execute_input":"2023-11-26T10:59:03.185966Z","iopub.status.idle":"2023-11-26T10:59:05.580047Z","shell.execute_reply.started":"2023-11-26T10:59:03.185930Z","shell.execute_reply":"2023-11-26T10:59:05.578850Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[{'val_loss': 4.605810642242432, 'val_acc': 0.009903847239911556}]"},"metadata":{}}]},{"cell_type":"code","source":"h += fit_one_cycle(int(epochs/4), max_lr, model, trainloader, testloader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T10:59:05.582017Z","iopub.execute_input":"2023-11-26T10:59:05.582328Z","iopub.status.idle":"2023-11-26T11:11:39.823789Z","shell.execute_reply.started":"2023-11-26T10:59:05.582296Z","shell.execute_reply":"2023-11-26T11:11:39.822668Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch [0], last_lr: 0.00007, train_loss: 3.6116, val_loss: 3.0522, val_acc: 0.2601\nEpoch [1], last_lr: 0.00015, train_loss: 2.8721, val_loss: 2.7676, val_acc: 0.3094\nEpoch [2], last_lr: 0.00028, train_loss: 2.5340, val_loss: 2.5766, val_acc: 0.3416\nEpoch [3], last_lr: 0.00044, train_loss: 2.2905, val_loss: 2.4500, val_acc: 0.3811\nEpoch [4], last_lr: 0.00060, train_loss: 2.1004, val_loss: 2.4354, val_acc: 0.3817\nEpoch [5], last_lr: 0.00076, train_loss: 1.9111, val_loss: 2.2123, val_acc: 0.4408\nEpoch [6], last_lr: 0.00089, train_loss: 1.7705, val_loss: 2.4124, val_acc: 0.4156\nEpoch [7], last_lr: 0.00097, train_loss: 1.6341, val_loss: 2.0725, val_acc: 0.4588\nEpoch [8], last_lr: 0.00100, train_loss: 1.5078, val_loss: 2.3241, val_acc: 0.4178\nEpoch [9], last_lr: 0.00099, train_loss: 1.3848, val_loss: 1.9250, val_acc: 0.4932\nEpoch [10], last_lr: 0.00098, train_loss: 1.2841, val_loss: 1.6345, val_acc: 0.5584\nEpoch [11], last_lr: 0.00095, train_loss: 1.1955, val_loss: 1.8463, val_acc: 0.5095\nEpoch [12], last_lr: 0.00091, train_loss: 1.1044, val_loss: 1.6951, val_acc: 0.5525\nEpoch [13], last_lr: 0.00087, train_loss: 1.0412, val_loss: 1.6529, val_acc: 0.5486\nEpoch [14], last_lr: 0.00081, train_loss: 0.9667, val_loss: 1.5573, val_acc: 0.5734\nEpoch [15], last_lr: 0.00075, train_loss: 0.8916, val_loss: 1.5254, val_acc: 0.5833\nEpoch [16], last_lr: 0.00068, train_loss: 0.8219, val_loss: 1.4249, val_acc: 0.6158\nEpoch [17], last_lr: 0.00061, train_loss: 0.7476, val_loss: 1.3483, val_acc: 0.6255\nEpoch [18], last_lr: 0.00054, train_loss: 0.6656, val_loss: 1.2335, val_acc: 0.6574\nEpoch [19], last_lr: 0.00046, train_loss: 0.5797, val_loss: 1.2295, val_acc: 0.6612\nEpoch [20], last_lr: 0.00039, train_loss: 0.4992, val_loss: 1.2370, val_acc: 0.6562\nEpoch [21], last_lr: 0.00032, train_loss: 0.4106, val_loss: 1.2428, val_acc: 0.6702\nEpoch [22], last_lr: 0.00025, train_loss: 0.3220, val_loss: 1.1524, val_acc: 0.6916\nEpoch [23], last_lr: 0.00019, train_loss: 0.2354, val_loss: 1.0905, val_acc: 0.7058\nEpoch [24], last_lr: 0.00013, train_loss: 0.1633, val_loss: 1.0343, val_acc: 0.7256\nEpoch [25], last_lr: 0.00009, train_loss: 0.1162, val_loss: 1.0052, val_acc: 0.7355\nEpoch [26], last_lr: 0.00005, train_loss: 0.0796, val_loss: 0.9998, val_acc: 0.7404\nEpoch [27], last_lr: 0.00002, train_loss: 0.0608, val_loss: 0.9936, val_acc: 0.7423\nEpoch [28], last_lr: 0.00001, train_loss: 0.0495, val_loss: 0.9935, val_acc: 0.7437\nEpoch [29], last_lr: 0.00000, train_loss: 0.0454, val_loss: 0.9929, val_acc: 0.7443\n","output_type":"stream"}]},{"cell_type":"code","source":"# Fitting the second 1/4 epochs\nh += fit_one_cycle(int(epochs/4), max_lr/10, model, trainloader, testloader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:11:39.827570Z","iopub.execute_input":"2023-11-26T11:11:39.827963Z","iopub.status.idle":"2023-11-26T11:24:14.577828Z","shell.execute_reply.started":"2023-11-26T11:11:39.827930Z","shell.execute_reply":"2023-11-26T11:24:14.576733Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Epoch [0], last_lr: 0.00001, train_loss: 0.0454, val_loss: 0.9952, val_acc: 0.7430\nEpoch [1], last_lr: 0.00002, train_loss: 0.0440, val_loss: 1.0023, val_acc: 0.7461\nEpoch [2], last_lr: 0.00003, train_loss: 0.0437, val_loss: 1.0168, val_acc: 0.7406\nEpoch [3], last_lr: 0.00004, train_loss: 0.0472, val_loss: 1.0521, val_acc: 0.7352\nEpoch [4], last_lr: 0.00006, train_loss: 0.0516, val_loss: 1.0967, val_acc: 0.7264\nEpoch [5], last_lr: 0.00008, train_loss: 0.0660, val_loss: 1.1268, val_acc: 0.7220\nEpoch [6], last_lr: 0.00009, train_loss: 0.0851, val_loss: 1.1660, val_acc: 0.7052\nEpoch [7], last_lr: 0.00010, train_loss: 0.0957, val_loss: 1.1777, val_acc: 0.7057\nEpoch [8], last_lr: 0.00010, train_loss: 0.0980, val_loss: 1.2845, val_acc: 0.6933\nEpoch [9], last_lr: 0.00010, train_loss: 0.0941, val_loss: 1.2104, val_acc: 0.7070\nEpoch [10], last_lr: 0.00010, train_loss: 0.0831, val_loss: 1.1788, val_acc: 0.7093\nEpoch [11], last_lr: 0.00010, train_loss: 0.0704, val_loss: 1.2001, val_acc: 0.7109\nEpoch [12], last_lr: 0.00009, train_loss: 0.0685, val_loss: 1.2076, val_acc: 0.7114\nEpoch [13], last_lr: 0.00009, train_loss: 0.0564, val_loss: 1.1633, val_acc: 0.7197\nEpoch [14], last_lr: 0.00008, train_loss: 0.0492, val_loss: 1.1712, val_acc: 0.7214\nEpoch [15], last_lr: 0.00008, train_loss: 0.0430, val_loss: 1.1492, val_acc: 0.7234\nEpoch [16], last_lr: 0.00007, train_loss: 0.0344, val_loss: 1.1190, val_acc: 0.7306\nEpoch [17], last_lr: 0.00006, train_loss: 0.0290, val_loss: 1.1194, val_acc: 0.7368\nEpoch [18], last_lr: 0.00005, train_loss: 0.0247, val_loss: 1.1225, val_acc: 0.7341\nEpoch [19], last_lr: 0.00005, train_loss: 0.0207, val_loss: 1.0872, val_acc: 0.7409\nEpoch [20], last_lr: 0.00004, train_loss: 0.0177, val_loss: 1.0891, val_acc: 0.7347\nEpoch [21], last_lr: 0.00003, train_loss: 0.0147, val_loss: 1.0850, val_acc: 0.7395\nEpoch [22], last_lr: 0.00003, train_loss: 0.0124, val_loss: 1.0739, val_acc: 0.7418\nEpoch [23], last_lr: 0.00002, train_loss: 0.0113, val_loss: 1.0655, val_acc: 0.7418\nEpoch [24], last_lr: 0.00001, train_loss: 0.0102, val_loss: 1.0625, val_acc: 0.7443\nEpoch [25], last_lr: 0.00001, train_loss: 0.0095, val_loss: 1.0659, val_acc: 0.7432\nEpoch [26], last_lr: 0.00000, train_loss: 0.0088, val_loss: 1.0616, val_acc: 0.7426\nEpoch [27], last_lr: 0.00000, train_loss: 0.0086, val_loss: 1.0601, val_acc: 0.7447\nEpoch [28], last_lr: 0.00000, train_loss: 0.0080, val_loss: 1.0601, val_acc: 0.7445\nEpoch [29], last_lr: 0.00000, train_loss: 0.0082, val_loss: 1.0608, val_acc: 0.7436\n","output_type":"stream"}]},{"cell_type":"code","source":"\nh += fit_one_cycle(int(epochs/8), max_lr/100, model, trainloader, testloader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:24:14.582559Z","iopub.execute_input":"2023-11-26T11:24:14.582908Z","iopub.status.idle":"2023-11-26T11:30:31.739854Z","shell.execute_reply.started":"2023-11-26T11:24:14.582874Z","shell.execute_reply":"2023-11-26T11:30:31.738797Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Epoch [0], last_lr: 0.00000, train_loss: 0.0082, val_loss: 1.0608, val_acc: 0.7451\nEpoch [1], last_lr: 0.00000, train_loss: 0.0080, val_loss: 1.0618, val_acc: 0.7443\nEpoch [2], last_lr: 0.00001, train_loss: 0.0084, val_loss: 1.0647, val_acc: 0.7421\nEpoch [3], last_lr: 0.00001, train_loss: 0.0090, val_loss: 1.0627, val_acc: 0.7428\nEpoch [4], last_lr: 0.00001, train_loss: 0.0095, val_loss: 1.0679, val_acc: 0.7419\nEpoch [5], last_lr: 0.00001, train_loss: 0.0093, val_loss: 1.0611, val_acc: 0.7422\nEpoch [6], last_lr: 0.00001, train_loss: 0.0097, val_loss: 1.0530, val_acc: 0.7458\nEpoch [7], last_lr: 0.00001, train_loss: 0.0091, val_loss: 1.0553, val_acc: 0.7464\nEpoch [8], last_lr: 0.00001, train_loss: 0.0081, val_loss: 1.0506, val_acc: 0.7448\nEpoch [9], last_lr: 0.00000, train_loss: 0.0081, val_loss: 1.0514, val_acc: 0.7446\nEpoch [10], last_lr: 0.00000, train_loss: 0.0077, val_loss: 1.0490, val_acc: 0.7437\nEpoch [11], last_lr: 0.00000, train_loss: 0.0074, val_loss: 1.0463, val_acc: 0.7437\nEpoch [12], last_lr: 0.00000, train_loss: 0.0073, val_loss: 1.0485, val_acc: 0.7440\nEpoch [13], last_lr: 0.00000, train_loss: 0.0071, val_loss: 1.0478, val_acc: 0.7433\nEpoch [14], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0466, val_acc: 0.7435\n","output_type":"stream"}]},{"cell_type":"code","source":"\nh += fit_one_cycle(int(epochs/8), max_lr/1000, model, trainloader, testloader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:30:31.741327Z","iopub.execute_input":"2023-11-26T11:30:31.741637Z","iopub.status.idle":"2023-11-26T11:36:49.605591Z","shell.execute_reply.started":"2023-11-26T11:30:31.741593Z","shell.execute_reply":"2023-11-26T11:36:49.604442Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Epoch [0], last_lr: 0.00000, train_loss: 0.0072, val_loss: 1.0471, val_acc: 0.7434\nEpoch [1], last_lr: 0.00000, train_loss: 0.0074, val_loss: 1.0472, val_acc: 0.7435\nEpoch [2], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0478, val_acc: 0.7441\nEpoch [3], last_lr: 0.00000, train_loss: 0.0071, val_loss: 1.0472, val_acc: 0.7444\nEpoch [4], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0473, val_acc: 0.7438\nEpoch [5], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0478, val_acc: 0.7433\nEpoch [6], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0484, val_acc: 0.7429\nEpoch [7], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0461, val_acc: 0.7437\nEpoch [8], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0472, val_acc: 0.7439\nEpoch [9], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0486, val_acc: 0.7435\nEpoch [10], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0490, val_acc: 0.7434\nEpoch [11], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0464, val_acc: 0.7438\nEpoch [12], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0478, val_acc: 0.7439\nEpoch [13], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0478, val_acc: 0.7450\nEpoch [14], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0474, val_acc: 0.7437\n","output_type":"stream"}]},{"cell_type":"code","source":"\nh += fit_one_cycle(int(epochs/4), max_lr/100, model, trainloader, testloader, \n                             grad_clip=grad_clip, \n                             weight_decay=weight_decay, \n                             opt_func=opt_func)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:36:49.608491Z","iopub.execute_input":"2023-11-26T11:36:49.608834Z","iopub.status.idle":"2023-11-26T11:49:24.361515Z","shell.execute_reply.started":"2023-11-26T11:36:49.608801Z","shell.execute_reply":"2023-11-26T11:49:24.360348Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Epoch [0], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0455, val_acc: 0.7449\nEpoch [1], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0460, val_acc: 0.7426\nEpoch [2], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0441, val_acc: 0.7436\nEpoch [3], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0475, val_acc: 0.7437\nEpoch [4], last_lr: 0.00001, train_loss: 0.0077, val_loss: 1.0443, val_acc: 0.7440\nEpoch [5], last_lr: 0.00001, train_loss: 0.0080, val_loss: 1.0456, val_acc: 0.7431\nEpoch [6], last_lr: 0.00001, train_loss: 0.0079, val_loss: 1.0507, val_acc: 0.7424\nEpoch [7], last_lr: 0.00001, train_loss: 0.0081, val_loss: 1.0484, val_acc: 0.7426\nEpoch [8], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0503, val_acc: 0.7419\nEpoch [9], last_lr: 0.00001, train_loss: 0.0089, val_loss: 1.0508, val_acc: 0.7409\nEpoch [10], last_lr: 0.00001, train_loss: 0.0088, val_loss: 1.0502, val_acc: 0.7456\nEpoch [11], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0479, val_acc: 0.7419\nEpoch [12], last_lr: 0.00001, train_loss: 0.0091, val_loss: 1.0453, val_acc: 0.7418\nEpoch [13], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0452, val_acc: 0.7450\nEpoch [14], last_lr: 0.00001, train_loss: 0.0088, val_loss: 1.0409, val_acc: 0.7437\nEpoch [15], last_lr: 0.00001, train_loss: 0.0080, val_loss: 1.0447, val_acc: 0.7444\nEpoch [16], last_lr: 0.00001, train_loss: 0.0081, val_loss: 1.0380, val_acc: 0.7439\nEpoch [17], last_lr: 0.00001, train_loss: 0.0079, val_loss: 1.0352, val_acc: 0.7443\nEpoch [18], last_lr: 0.00001, train_loss: 0.0078, val_loss: 1.0377, val_acc: 0.7461\nEpoch [19], last_lr: 0.00000, train_loss: 0.0077, val_loss: 1.0367, val_acc: 0.7438\nEpoch [20], last_lr: 0.00000, train_loss: 0.0072, val_loss: 1.0382, val_acc: 0.7458\nEpoch [21], last_lr: 0.00000, train_loss: 0.0072, val_loss: 1.0337, val_acc: 0.7451\nEpoch [22], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0357, val_acc: 0.7460\nEpoch [23], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0336, val_acc: 0.7442\nEpoch [24], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0342, val_acc: 0.7462\nEpoch [25], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0319, val_acc: 0.7467\nEpoch [26], last_lr: 0.00000, train_loss: 0.0065, val_loss: 1.0315, val_acc: 0.7460\nEpoch [27], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0311, val_acc: 0.7452\nEpoch [28], last_lr: 0.00000, train_loss: 0.0063, val_loss: 1.0326, val_acc: 0.7448\nEpoch [29], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0330, val_acc: 0.7446\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Prediction","metadata":{}},{"cell_type":"code","source":"# Saving the model to h5 file\ntorch.save(model.state_dict(), 'img_classification_cifar.h5')","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:49:24.365293Z","iopub.execute_input":"2023-11-26T11:49:24.366266Z","iopub.status.idle":"2023-11-26T11:49:24.599447Z","shell.execute_reply.started":"2023-11-26T11:49:24.366217Z","shell.execute_reply":"2023-11-26T11:49:24.598449Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Generate testing accuracy, predicted label, confusion matrix, and table for classification report\ndef test_label_predictions(model, device, test_loader):\n    model.eval()\n    actuals = []\n    predictions = []\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            prediction = output.argmax(dim=1, keepdim=True)\n            actuals.extend(target.view_as(prediction))\n            predictions.extend(prediction)\n    return [i.item() for i in actuals], [i.item() for i in predictions]\n\ny_test, y_pred = test_label_predictions(model, device, testloader)\ncm=confusion_matrix(y_test, y_pred)\ncr=classification_report(y_test, y_pred)\nfs=f1_score(y_test,y_pred,average='weighted')\nrs=recall_score(y_test, y_pred,average='weighted')\naccuracy=accuracy_score(y_test, y_pred)\nprint('Confusion matrix:')\nprint(cm)\nprint(cr)\nprint('F1 score: %f' % fs)\nprint('Recall score: %f' % rs)\nprint('Accuracy score: %f' % accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-11-26T11:49:24.600718Z","iopub.execute_input":"2023-11-26T11:49:24.600978Z","iopub.status.idle":"2023-11-26T11:49:27.210166Z","shell.execute_reply.started":"2023-11-26T11:49:24.600955Z","shell.execute_reply":"2023-11-26T11:49:27.209107Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Confusion matrix:\n[[88  0  0 ...  0  0  0]\n [ 0 90  0 ...  0  0  0]\n [ 1  1 60 ...  0  4  0]\n ...\n [ 0  0  0 ... 78  0  0]\n [ 1  0  2 ...  0 63  0]\n [ 0  0  0 ...  0  0 75]]\n              precision    recall  f1-score   support\n\n           0       0.90      0.88      0.89       100\n           1       0.79      0.90      0.84       100\n           2       0.67      0.60      0.63       100\n           3       0.67      0.52      0.58       100\n           4       0.57      0.67      0.61       100\n           5       0.81      0.76      0.78       100\n           6       0.75      0.79      0.77       100\n           7       0.80      0.80      0.80       100\n           8       0.91      0.86      0.89       100\n           9       0.84      0.80      0.82       100\n          10       0.62      0.55      0.58       100\n          11       0.59      0.58      0.59       100\n          12       0.80      0.83      0.81       100\n          13       0.76      0.65      0.70       100\n          14       0.69      0.73      0.71       100\n          15       0.79      0.85      0.82       100\n          16       0.69      0.77      0.73       100\n          17       0.90      0.88      0.89       100\n          18       0.73      0.66      0.69       100\n          19       0.72      0.68      0.70       100\n          20       0.88      0.88      0.88       100\n          21       0.80      0.94      0.86       100\n          22       0.74      0.75      0.74       100\n          23       0.85      0.83      0.84       100\n          24       0.84      0.83      0.83       100\n          25       0.69      0.62      0.65       100\n          26       0.67      0.68      0.67       100\n          27       0.68      0.68      0.68       100\n          28       0.79      0.81      0.80       100\n          29       0.73      0.71      0.72       100\n          30       0.70      0.63      0.66       100\n          31       0.73      0.77      0.75       100\n          32       0.71      0.67      0.69       100\n          33       0.69      0.65      0.67       100\n          34       0.72      0.75      0.74       100\n          35       0.54      0.52      0.53       100\n          36       0.86      0.84      0.85       100\n          37       0.79      0.76      0.78       100\n          38       0.70      0.65      0.67       100\n          39       0.91      0.87      0.89       100\n          40       0.65      0.66      0.65       100\n          41       0.92      0.88      0.90       100\n          42       0.72      0.73      0.72       100\n          43       0.89      0.81      0.85       100\n          44       0.53      0.52      0.52       100\n          45       0.65      0.69      0.67       100\n          46       0.59      0.57      0.58       100\n          47       0.65      0.69      0.67       100\n          48       0.88      0.94      0.91       100\n          49       0.80      0.89      0.84       100\n          50       0.58      0.62      0.60       100\n          51       0.73      0.73      0.73       100\n          52       0.63      0.67      0.65       100\n          53       0.84      0.92      0.88       100\n          54       0.81      0.88      0.85       100\n          55       0.52      0.48      0.50       100\n          56       0.95      0.87      0.91       100\n          57       0.82      0.81      0.81       100\n          58       0.83      0.88      0.85       100\n          59       0.66      0.69      0.67       100\n          60       0.85      0.88      0.87       100\n          61       0.72      0.67      0.69       100\n          62       0.72      0.81      0.76       100\n          63       0.74      0.69      0.72       100\n          64       0.71      0.59      0.64       100\n          65       0.58      0.59      0.58       100\n          66       0.86      0.82      0.84       100\n          67       0.69      0.66      0.67       100\n          68       0.90      0.92      0.91       100\n          69       0.84      0.80      0.82       100\n          70       0.73      0.80      0.77       100\n          71       0.79      0.87      0.83       100\n          72       0.48      0.50      0.49       100\n          73       0.56      0.55      0.56       100\n          74       0.55      0.62      0.58       100\n          75       0.86      0.90      0.88       100\n          76       0.90      0.90      0.90       100\n          77       0.78      0.66      0.71       100\n          78       0.63      0.63      0.63       100\n          79       0.74      0.79      0.76       100\n          80       0.68      0.62      0.65       100\n          81       0.74      0.76      0.75       100\n          82       0.95      0.91      0.93       100\n          83       0.70      0.73      0.71       100\n          84       0.71      0.70      0.71       100\n          85       0.79      0.84      0.81       100\n          86       0.77      0.72      0.74       100\n          87       0.76      0.84      0.80       100\n          88       0.86      0.80      0.83       100\n          89       0.79      0.86      0.82       100\n          90       0.77      0.85      0.81       100\n          91       0.78      0.83      0.81       100\n          92       0.70      0.62      0.66       100\n          93       0.59      0.58      0.58       100\n          94       0.89      0.95      0.92       100\n          95       0.72      0.71      0.71       100\n          96       0.72      0.68      0.70       100\n          97       0.78      0.78      0.78       100\n          98       0.62      0.63      0.62       100\n          99       0.75      0.75      0.75       100\n\n    accuracy                           0.74     10000\n   macro avg       0.74      0.74      0.74     10000\nweighted avg       0.74      0.74      0.74     10000\n\nF1 score: 0.742467\nRecall score: 0.743400\nAccuracy score: 0.743400\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}